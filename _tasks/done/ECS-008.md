# ECS-008: Volume operations (EFS)

**Component:** ECS (AWS ECS Fargate backend)
**Phase:** 2
**Depends on:** ECS-001
**Estimated effort:** M

---

## Description

Implement volume operations for the ECS backend: create, list, inspect, and remove. Docker volumes map to EFS access points on a pre-provisioned EFS filesystem. CI runners create named volumes for sharing data between containers (e.g., helper clones code into a build volume, build container reads it). Multiple Fargate tasks mount the same EFS access point to achieve shared filesystem semantics. This task manages EFS access point lifecycle, records volume-to-access-point mappings, and integrates with the task definition builder (ECS-002) to apply volume mounts at container launch time.

---

## Context

### Volume emulation overview (Spec Section 10.1)

> GitLab Runner creates volumes for:
> - `/builds/<namespace>/<project>` -- shared between helper and build containers (git clone, build artifacts)
> - `/cache` -- persistent cache across jobs
> - Additional user-configured volumes
>
> Both containers mount the same volume for data sharing. This is critical for the helper -> build container handoff pattern.

### Volume strategy (Spec Section 10.2)

> | Volume Type | Cloud Emulation | Used For |
> |---|---|---|
> | Named volumes (Docker `VolumeCreate`) | Cloud shared filesystem (EFS, GCS, Azure Files) | Caches |
> | Bind mounts (from `HostConfig.Binds`) | Cloud shared filesystem mounted at specified path | Build data sharing |
> | `VolumesFrom` (inherit from another container) | Same shared filesystem mount applied to multiple tasks | Helper <-> Build sharing |
> | Anonymous volumes | Ephemeral task storage | Temp data |

### ECS + EFS implementation (Spec Section 10.3)

> #### AWS ECS + EFS
> 1. Create an EFS filesystem (or use a pre-provisioned one) per sockerless instance
> 2. Create EFS access points for each named volume
> 3. Mount the EFS access point in the ECS task definition
> 4. Multiple tasks (containers) can mount the same access point -> shared data

### Volume lifecycle (Spec Section 10.4)

> | Docker Operation | Sockerless Action |
> |---|---|
> | `POST /volumes/create` | Create cloud storage resource (or logical directory in shared filesystem) |
> | Bind mount in `POST /containers/create` | Record mount mapping; apply when launching cloud task |
> | `VolumesFrom` in `POST /containers/create` | Copy mount list from referenced container's config |
> | `DELETE /volumes/{name}` | Delete cloud storage resource |

### Volume sharing between containers (Spec Section 13.3, Gap 6)

> **Problem:** Helper and build containers share the same Docker volumes (for git clone -> build handoff). Cloud containers are isolated tasks -- they don't share local filesystems.
>
> **Solution:** Cloud shared filesystems:
> - **ECS:** EFS filesystem with access points. Multiple Fargate tasks mount the same EFS access point -> shared data.
>
> Volume creation (`POST /volumes/create`) provisions a directory/access-point in the shared filesystem. Container creation records the volume mount mapping. Container start applies the mount to the cloud task definition.
>
> **Performance consideration:** Cloud shared filesystems add latency vs. local Docker volumes. EFS latency is 0.5-5ms per operation. For CI workloads (git clone, build), this is acceptable.

### Docker volume create request (Spec Section 4.6)

> ```json
> {
>   "Name": "runner-cache-sha256abc",
>   "Driver": "local",
>   "Labels": {"com.gitlab.gitlab-runner.managed": "true"}
> }
> ```
>
> **Response:**
> ```json
> {
>   "Name": "runner-cache-sha256abc",
>   "Driver": "local",
>   "Mountpoint": "/var/lib/docker/volumes/runner-cache-sha256abc/_data",
>   "Labels": {"com.gitlab.gitlab-runner.managed": "true"},
>   "Scope": "local",
>   "CreatedAt": "2026-02-15T12:00:00Z"
> }
> ```

### ECS backend configuration (Spec Section 15.3)

> ```yaml
> ecs:
>   efs_filesystem_id: "fs-abc123"
> ```

---

## Acceptance Criteria

### VolumeCreate
1. `VolumeCreate` creates an EFS access point on the configured EFS filesystem (`efs_filesystem_id`). The access point root directory path is `/sockerless/volumes/<volume-name>`.
2. The access point is created with POSIX user `uid: 0, gid: 0` and root directory creation info `owner_uid: 0, owner_gid: 0, permissions: "0755"` so that containers running as root can read/write.
3. The access point is tagged with `sockerless-volume: <volume-name>` for identification and cleanup.
4. The volume is stored in state with: name, EFS access point ID, EFS filesystem ID, labels, creation timestamp, and a synthetic mountpoint (`/var/lib/docker/volumes/<name>/_data` for Docker-compatible inspect output).
5. `VolumeCreate` returns the volume metadata in Docker's format (Name, Driver, Mountpoint, Labels, Scope, CreatedAt).
6. If a volume with the same name already exists, return the existing volume (idempotent, matching Docker's behavior).
7. If `efs_filesystem_id` is not configured, volumes are created in state only (logical volumes with no backing storage) and a warning is logged. This allows the backend to run without EFS for testing.

### Volume mount in task definition
8. When a container is created with `HostConfig.Binds` (e.g., `["runner-cache:/cache:rw"]`), the volume name is resolved to an EFS access point and added to the task definition as an EFS volume with a corresponding container mount point.
9. The task definition volume is configured with `efsVolumeConfiguration`: `fileSystemId`, `transitEncryption: "ENABLED"`, `authorizationConfig.accessPointId`, and `rootDirectory: "/"` (access point root).
10. The container definition mount includes `sourceVolume` (matching the task definition volume name), `containerPath` (the mount target inside the container), and `readOnly` (from the bind mount options).
11. When a container is created with `HostConfig.VolumesFrom` (e.g., `["helper-container-id"]`), the bind mounts from the referenced container are copied to the new container's task definition.

### VolumeInspect
12. `VolumeInspect` returns the volume metadata from state in Docker's format.
13. Lookup by volume name. Returns 404 if the volume does not exist.

### VolumeList
14. `VolumeList` returns all volumes from the state store.
15. The response includes the `Volumes` array and a `Warnings` array (empty).
16. Filters are supported: `name`, `label` (key, key=value), `driver`.

### VolumeRemove
17. `VolumeRemove` deletes the EFS access point and removes the volume from the state store.
18. If `Force` is false and the volume is in use by a running container, return `409 Conflict` with a descriptive message.
19. If `Force` is true, remove the volume regardless of usage. The running containers will lose access to the mount (EFS behavior: access point deletion does not unmount active mounts, but new mount attempts will fail).
20. The EFS access point directory contents are NOT deleted when the access point is removed (EFS retains the data on the filesystem). Document this behavior.

### Error handling
21. If `efs_filesystem_id` is not configured and a volume create is requested, create a logical volume in state (no EFS access point) and log a warning that volume sharing between containers will not work.
22. If the EFS filesystem is not accessible (wrong ID, permissions, network), return a 500 error with the AWS error details.

### Example SDK Calls

```go
// CreateAccessPoint
efs.CreateAccessPoint(&efs.CreateAccessPointInput{
    FileSystemId: &fsId,
    PosixUser: &types.PosixUser{
        Uid: aws.Int64(0),
        Gid: aws.Int64(0),
    },
    RootDirectory: &types.RootDirectory{
        Path: aws.String("/sockerless/volumes/" + volumeName),
        CreationInfo: &types.CreationInfo{
            OwnerUid:    aws.Int64(0),
            OwnerGid:    aws.Int64(0),
            Permissions: aws.String("0755"),
        },
    },
    Tags: []types.Tag{
        {Key: aws.String("sockerless-volume"), Value: &volumeName},
    },
})

// DeleteAccessPoint
efs.DeleteAccessPoint(&efs.DeleteAccessPointInput{
    AccessPointId: &accessPointId,
})

// Task definition EFS volume configuration
ecsTypes.Volume{
    Name: aws.String("vol-" + volumeName),
    EfsVolumeConfiguration: &ecsTypes.EFSVolumeConfiguration{
        FileSystemId:      &fsId,
        TransitEncryption: ecsTypes.EFSTransitEncryptionEnabled,
        AuthorizationConfig: &ecsTypes.EFSAuthorizationConfig{
            AccessPointId: &accessPointId,
        },
        RootDirectory: aws.String("/"),
    },
}
```

---

## Definition of Done

### Code Quality
- [ ] Code compiles: `go build ./...` passes with zero errors
- [ ] Lint passes: `golangci-lint run ./...` with zero warnings
- [ ] Vet passes: `go vet ./...` with zero warnings
- [ ] Race detector: `go test -race ./...` passes
- [ ] No new lint warnings introduced

### Testing
- [ ] Unit tests with mock AWS clients for: access point creation (with correct path, POSIX user, tags), access point deletion, volume inspect, volume list with filters, volume remove (in-use check, force), task definition volume generation, idempotent create
- [ ] Unit tests for: bind mount parsing (`name:/path:rw`), VolumesFrom resolution, logical volume creation (no EFS)
- [ ] Tests are deterministic (no flaky tests)

### Error Handling
- [ ] Volume not found returns 404 with Docker-format message
- [ ] Remove in-use volume without force returns 409
- [ ] EFS API errors wrapped with context and descriptive message
- [ ] Missing `efs_filesystem_id` logs warning and creates logical volume
- [ ] Errors wrapped with context: `fmt.Errorf("ecs: volume create: %w", err)`
- [ ] Zerolog structured logging with volume name, access point ID, filesystem ID

### Documentation
- [ ] GoDoc comments on all exported types, functions, methods, and constants
- [ ] README updated with EFS prerequisites (filesystem creation, mount targets, security groups)

### Integration
- [ ] Volume mounts applied to task definitions in container create (ECS-002)
- [ ] VolumesFrom copies mounts from referenced container state
- [ ] Volume in-use tracking via container state (container references volume by name)
- [ ] EFS access point IDs stored in state for cleanup on volume remove

---

## Suggested File Paths

```
backends/ecs/
├── volume_create.go            # VolumeCreate: EFS access point creation, state storage
├── volume_inspect.go           # VolumeInspect and VolumeList: state lookup, Docker format
├── volume_remove.go            # VolumeRemove: access point deletion, in-use check
├── volume_mount.go             # Bind mount parsing, VolumesFrom resolution, task def integration
└── store.go                    # (update) Volume state: name, access point ID, container usage tracking
```

---

## Notes

- EFS access points are limited to 1,000 per filesystem. For high-volume CI workloads, monitor access point usage and clean up aggressively. Consider warning when usage exceeds 80%.
- EFS mount targets must exist in each subnet where Fargate tasks run. This is a deployment prerequisite -- the EFS filesystem must have mount targets in the configured subnets. Without mount targets, tasks will fail to start with a mount error.
- Transit encryption (`transitEncryption: ENABLED`) is recommended for security. It adds minimal latency (< 1ms).
- EFS performance: General Purpose mode is sufficient for CI workloads. Throughput scales with filesystem size (bursting). For very active CI systems, consider Provisioned Throughput.
- The access point root directory (`/sockerless/volumes/<name>`) is created automatically by EFS if it doesn't exist, using the `CreationInfo` settings. This means volume creation is effectively instantaneous.
- Bind mount format parsing: Docker sends binds as `source:target:options` where source is either a host path (starts with `/`) or a volume name. For the ECS backend, host paths are silently accepted but not applied (document this). Volume names are resolved to EFS access points.
- The `/var/run/docker.sock` bind mount from GitHub Actions Runner should be silently accepted and ignored (Spec Section 13.3, Gap 7).
- Anonymous volumes (specified in the image's `VOLUME` directive but not explicitly named) use ephemeral task storage. They do not need EFS access points.
- Consider implementing periodic reconciliation: compare state store volumes with EFS access points to detect and clean up orphaned access points.
